{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Import-modules-and-functions\" data-toc-modified-id=\"Import-modules-and-functions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Import modules and functions</a></div><div class=\"lev1 toc-item\"><a href=\"#Input-fixed-variables-and-datapaths\" data-toc-modified-id=\"Input-fixed-variables-and-datapaths-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Input fixed variables and datapaths</a></div><div class=\"lev1 toc-item\"><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Import data</a></div><div class=\"lev1 toc-item\"><a href=\"#Classify-receiver-clusters\" data-toc-modified-id=\"Classify-receiver-clusters-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Classify receiver clusters</a></div><div class=\"lev1 toc-item\"><a href=\"#Check-the-classification:-3-fold-cross-validation\" data-toc-modified-id=\"Check-the-classification:-3-fold-cross-validation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Check the classification: 3-fold cross validation</a></div><div class=\"lev1 toc-item\"><a href=\"#Filter-fish-positions-based-on-receiver-cluster-classification\" data-toc-modified-id=\"Filter-fish-positions-based-on-receiver-cluster-classification-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Filter fish positions based on receiver cluster classification</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an example of how to use the function \"classify_receiver_clusters\".   \n",
    "A **cluster** is a group of at least three receivers used to calculate a position.   \n",
    "Clusters are classified as **well** or **badly** performing, according to their ability to correctly position fixed tags.   \n",
    "A cluster is only classified if it is used to calculate at least 10 fixed tag positions.   \n",
    "\n",
    "Before using the function, you have to define:\n",
    "- an **accuracy objective** = the acceptable distance between calculated and real fish position\n",
    "- a **confidence level** = the percentage of fixed tag positions falling within the accuracy objective for a well performing cluster.\n",
    "\n",
    "For a confidence level of 95% applies:\n",
    "\n",
    "If at least 95% of the fixed tag positions calculated by a cluster, fall within the accuracy objective, the cluster is classified as well performing. If less than 95% fall within this objective, the cluster is classified as badly performing. \n",
    "\n",
    "Only fish positions calculated by well performing clusters are retained for behaviour analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:14.834888Z",
     "start_time": "2017-06-30T10:34:14.248749Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from classify_receiver_clusters import classify_receiver_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:14.843727Z",
     "start_time": "2017-06-30T10:34:14.837165Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_percentage(to_classify, performers_list):\n",
    "    \"\"\"\n",
    "    Function to calculate the percentage of good/bad performers in a deployment\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    to_classify = series with URX-groups used in a certain deployment (or dataset)\n",
    "    performers_list = list of bad or good performers\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    percentage\n",
    "    \"\"\"\n",
    "    return sum(to_classify.isin(performers_list))/len(to_classify)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:14.975204Z",
     "start_time": "2017-06-30T10:34:14.921824Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def URX_filtering_stats(pos_to_filter, known_pos, levels, accuracy_goal, min_nb_in_group=10):\n",
    "    \"\"\"\n",
    "    Calculate percentages of excluded, included and unclassified positions for data after the classification of the receiver clusters, and also false neg/pos/neutrals.\n",
    "    The filtering is based on data with known positions, of which the URX groups are classified for a given confidence level.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pos_to_filter = dataframe with positioning data needing to be filtered\n",
    "    known_pos = dataframe with positions with known HPEm (error), used to classify the receiver clusters\n",
    "    levels = list of confidence levels (e.g. [0.7, 0.8, 0.9])\n",
    "    accuracy_goal = maximum acceptable error (e.g. 2.5)\n",
    "    min_nb_in_group = minimum number of positions calculated by a URX-cluster to allow it to be classified (default 10)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dataframe with confidence levels in index and percentages in columns\n",
    "    \"\"\"\n",
    "    keys = ['excluded', 'false_neg', 'f_neg_tot', 'included', 'false_pos', 'f_pos_tot', 'unclassified', 'false_neutral', 'f_neutr_tot', 'avg_HPEm', 'med_HPEm', 'quant_95']\n",
    "    stats = {key: {} for key in keys}\n",
    "\n",
    "    for lim in levels:\n",
    "        URX_groups, good_perf, bad_perf = classify_receiver_clusters(known_pos, accuracy_goal, lim, min_group_size = min_nb_in_group)\n",
    "        \n",
    "        excluded_pos = pos_to_filter[pos_to_filter['URX'].isin(bad_perf)]\n",
    "        stats['excluded'][lim] = len(excluded_pos)/len(pos_to_filter)*100\n",
    "        if len(excluded_pos) > 0:\n",
    "            stats['false_neg'][lim] = len(excluded_pos[excluded_pos.HPEm<=2.5])/len(excluded_pos)*100\n",
    "        else:\n",
    "            stats['false_neg'][lim] = None\n",
    "        \n",
    "        stats['f_neg_tot'][lim] = len(excluded_pos[excluded_pos.HPEm<=2.5])/len(pos_to_filter)*100\n",
    "\n",
    "        included_pos = pos_to_filter[pos_to_filter['URX'].isin(good_perf)]\n",
    "        stats['included'][lim] = len(included_pos)/len(pos_to_filter)*100\n",
    "        if len(included_pos) > 0:\n",
    "            stats['false_pos'][lim] = len(included_pos[included_pos.HPEm>2.5])/len(included_pos)*100\n",
    "        else:\n",
    "            stats['false_pos'][lim] = None\n",
    "        \n",
    "        stats['f_pos_tot'][lim] = len(included_pos[included_pos.HPEm>2.5])/len(pos_to_filter)*100\n",
    "            \n",
    "        unclassified_pos = pos_to_filter[np.logical_not((pos_to_filter['URX'].isin(bad_perf)|(pos_to_filter['URX'].isin(good_perf))))]\n",
    "        stats['unclassified'][lim] = len(unclassified_pos)/len(pos_to_filter)*100\n",
    "        if len(unclassified_pos) > 0:\n",
    "            stats['false_neutral'][lim] = len(unclassified_pos[unclassified_pos.HPEm>2.5])/len(unclassified_pos)*100\n",
    "        else:\n",
    "            stats['false_neutral'][lim] = None\n",
    "            \n",
    "        stats['f_neutr_tot'][lim] = len(unclassified_pos[unclassified_pos.HPEm>2.5])/len(pos_to_filter)*100\n",
    "        \n",
    "        stats['avg_HPEm'][lim] = included_pos.HPEm.mean()\n",
    "        stats['med_HPEm'][lim] = included_pos.HPEm.median()\n",
    "        stats['quant_95'][lim] = included_pos.HPEm.quantile(0.95)\n",
    "\n",
    "            \n",
    "    return pd.DataFrame.from_dict(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:15.502143Z",
     "start_time": "2017-06-30T10:34:15.478873Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_fish_pos(fishdata, good_performers, bad_performers):\n",
    "    \"\"\"\n",
    "    Classify the fish positions as good, bad or unclassified, according to the classification \n",
    "    of the receiver clusters based on fixed tag data.\n",
    "    This function also writes out the results (percentages good, bad and unclassified positions).\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    fishdata = dataframe with fish positions and at least column URX\n",
    "    good_performers = list with good performing receiver clusters\n",
    "    bad_performers = list with bad performing receiver clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    fish_good = fishdata[fishdata.URX.isin(good_performers)]\n",
    "    fish_bad = fishdata[fishdata.URX.isin(bad_performers)]\n",
    "    fish_rest = fishdata[fishdata.URX.isin(good_performers+bad_performers)==False]\n",
    "    \n",
    "    print('Good positions: {:.2f}%'.format(len(fish_good)/len(fishdata)*100))\n",
    "    print('Bad positions: {:.2f}%'.format(len(fish_bad)/len(fishdata)*100))\n",
    "    print('Unclassified positions: {:.2f}%'.format(len(fish_rest)/len(fishdata)*100))\n",
    "\n",
    "    \n",
    "    return fish_good, fish_bad, fish_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:02:10.042965Z",
     "start_time": "2017-06-30T10:02:10.033028Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def three_fold_cross(dataset):\n",
    "    \"\"\"\n",
    "    Split dataset randomly in 3 parts.\n",
    "    \n",
    "    Inputs:\n",
    "    -------\n",
    "    dataset = pandas dataframe\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    3 new dataframes which are parts of the old dataframe\n",
    "    \"\"\"\n",
    "    nb_list = np.arange(len(dataset))\n",
    "    np.random.shuffle(nb_list)\n",
    "    \n",
    "    part1 = dataset.iloc[nb_list[:round(len(dataset)/3)],:]\n",
    "    part2 = dataset.iloc[nb_list[round(len(dataset)/3)+1:round(len(dataset)/3*2)],:]\n",
    "    part3 = dataset.iloc[nb_list[round(len(dataset)/3*2)+1:],:]\n",
    "    \n",
    "    return part1, part2, part3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input fixed variables and datapaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give the path to the csv-file which contains all calculated positions of fish tags and fixed tags (synchronization and reference tags). This file contains at least the columns TRANSMITTER, DATETIME, LAT, LON and URX (list of receivers used to calculate the position) for each position, and HPEm (absolute error) for fixed positions.   \n",
    "Give also the names of the fixed tags, as used in the TRANSMITTER column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:16.597487Z",
     "start_time": "2017-06-30T10:34:16.592254Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_vps_data = '/Users/jennavergeynst/Documents/Ham/VPS_01_20160304/data/positions/ALL-CALC-POSITIONS.csv'\n",
    "ref_sync = ['R1', 'R2', 'R3', 'S10', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S7', 'S8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:16.854004Z",
     "start_time": "2017-06-30T10:34:16.846239Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_goal = 2.5 #m\n",
    "confidence_level = 0.95\n",
    "min_nb_in_group = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:17.693683Z",
     "start_time": "2017-06-30T10:34:17.231566Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vps_data = pd.read_csv(path_to_vps_data, parse_dates=['DATETIME'], infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:17.731331Z",
     "start_time": "2017-06-30T10:34:17.696685Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fixed_tags = vps_data[vps_data.TRANSMITTER.isin(ref_sync)]\n",
    "fish_tags = vps_data[vps_data.TRANSMITTER.isin(ref_sync)==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify receiver clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:18.890750Z",
     "start_time": "2017-06-30T10:34:18.715721Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URX_groups, good_performers, bad_performers = classify_receiver_clusters(fixed_tags, accuracy_goal, confidence_level, min_nb_in_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:34:21.323208Z",
     "start_time": "2017-06-30T10:34:21.300457Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage bad performer clusters: 25.97\n",
      "Percentage good performer clusters: 7.14\n",
      "Percentage bad performers for fish positions: 23.05\n",
      "Percentage good performers for fish positions: 6.44\n"
     ]
    }
   ],
   "source": [
    "clusters = URX_groups['URX']\n",
    "fish_URX = pd.Series(fish_tags.URX.unique())\n",
    "\n",
    "print('Percentage bad performer clusters: {:.2f}'.format(classification_percentage(clusters, bad_performers)))\n",
    "print('Percentage good performer clusters: {:.2f}'.format(classification_percentage(clusters, good_performers)))\n",
    "print('Percentage bad performers for fish positions: {:.2f}'.format(classification_percentage(fish_URX, bad_performers)))\n",
    "print('Percentage good performers for fish positions: {:.2f}'.format(classification_percentage(fish_URX, good_performers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the classification: 3-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:02:35.026725Z",
     "start_time": "2017-06-30T10:02:34.961030Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "part1, part2, part3 = three_fold_cross(fixed_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:02:36.405251Z",
     "start_time": "2017-06-30T10:02:36.400247Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "levels = [0.7, 0.8, 0.9, 0.95, 0.99] # validate the classification for different confidence percentages\n",
    "cols = ['excluded', 'false_neg', 'f_neg_tot', 'included', 'false_pos', 'f_pos_tot', 'unclassified', 'false_neutral', 'f_neutr_tot', 'avg_HPEm', 'med_HPEm', 'quant_95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:02:41.356332Z",
     "start_time": "2017-06-30T10:02:40.955888Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URX_filtering_stats(part1, pd.concat([part2, part3]), levels, accuracy_goal, min_nb_in_group).loc[:,cols].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:02:55.361753Z",
     "start_time": "2017-06-30T10:02:54.955666Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URX_filtering_stats(part2, pd.concat([part1, part3]), levels, accuracy_goal, min_nb_in_group).loc[:,cols].round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:03:05.798614Z",
     "start_time": "2017-06-30T10:03:05.418314Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "URX_filtering_stats(part3, pd.concat([part1, part2]), levels, accuracy_goal, min_nb_in_group).loc[:,cols].round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter fish positions based on receiver cluster classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-30T10:20:23.886704Z",
     "start_time": "2017-06-30T10:20:23.868667Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fish_good, fish_bad, fish_rest = classify_fish_pos(fish_tags, good_performers, bad_performers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "2",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "844px",
   "left": "0px",
   "right": "1708px",
   "top": "107px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
